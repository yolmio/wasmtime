test run
set opt_level=none
set enable_simd32=true
target x86_64 has_avx512f has_avx512vl has_avx512bw has_avx512dq

function %avx512_masked_load_i32x16() -> i32 {
    ss0 = explicit_slot 64
    ss1 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16]
    stack_store v0, ss0
    v1 = vconst.i32x16 [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]
    v2 = vconst.i32x16 [-1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0]
    v3 = stack_addr.i64 ss0
    v4 = x86_simd_masked_load.i32x16 notrap aligned v2, v1, v3
    stack_store v4, ss1
    v5 = stack_load.i32 ss1
    v6 = stack_load.i32 ss1+4
    v7 = stack_load.i32 ss1+56
    v8 = stack_load.i32 ss1+60
    v9 = iadd v5, v6
    v10 = iadd v7, v8
    v11 = iadd v9, v10
    return v11
}
; run: %avx512_masked_load_i32x16() == 232

function %avx512_masked_load_i64x8() -> i64 {
    ss0 = explicit_slot 64
    ss1 = explicit_slot 64

block0:
    v0 = vconst.i64x8 [1 2 3 4 5 6 7 8]
    stack_store v0, ss0
    v1 = vconst.i64x8 [100 101 102 103 104 105 106 107]
    v2 = vconst.i64x8 [-1 0 -1 0 -1 0 -1 0]
    v3 = stack_addr.i64 ss0
    v4 = x86_simd_masked_load.i64x8 notrap aligned v2, v1, v3
    stack_store v4, ss1
    v5 = stack_load.i64 ss1
    v6 = stack_load.i64 ss1+8
    v7 = stack_load.i64 ss1+48
    v8 = stack_load.i64 ss1+56
    v9 = iadd v5, v6
    v10 = iadd v7, v8
    v11 = iadd v9, v10
    return v11
}
; run: %avx512_masked_load_i64x8() == 216

function %avx512_masked_store_i32x16() -> i32 {
    ss0 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    stack_store v0, ss0
    v1 = vconst.i32x16 [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]
    v2 = vconst.i32x16 [-1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0]
    v3 = stack_addr.i64 ss0
    x86_simd_masked_store.i32x16 notrap aligned v2, v1, v3
    v4 = stack_load.i32 ss0
    v5 = stack_load.i32 ss0+4
    v6 = stack_load.i32 ss0+56
    v7 = stack_load.i32 ss0+60
    v8 = iadd v4, v5
    v9 = iadd v6, v7
    v10 = iadd v8, v9
    return v10
}
; run: %avx512_masked_store_i32x16() == 230

function %avx512_gather_i32x16() -> i32 {
    ss0 = explicit_slot 64
    ss1 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25]
    stack_store v0, ss0
    v1 = vconst.i32x16 [15 0 14 1 13 2 12 3 11 4 10 5 9 6 8 7]
    v2 = stack_addr.i64 ss0
    v3 = x86_simd_gather.i32x16 notrap aligned v2, v1, 4
    stack_store v3, ss1
    v4 = stack_load.i32 ss1
    v5 = stack_load.i32 ss1+4
    v6 = stack_load.i32 ss1+56
    v7 = stack_load.i32 ss1+60
    v8 = iadd v4, v5
    v9 = iadd v6, v7
    v10 = iadd v8, v9
    return v10
}
; run: %avx512_gather_i32x16() == 70

function %avx512_gather_i64x8_i64x8() -> i64 {
    ss0 = explicit_slot 64
    ss1 = explicit_slot 64

block0:
    v0 = vconst.i64x8 [100 101 102 103 104 105 106 107]
    stack_store v0, ss0
    v1 = vconst.i64x8 [7 0 6 1 5 2 4 3]
    v2 = stack_addr.i64 ss0
    v3 = x86_simd_gather.i64x8 notrap aligned v2, v1, 8
    stack_store v3, ss1
    v4 = stack_load.i64 ss1
    v5 = stack_load.i64 ss1+8
    v6 = stack_load.i64 ss1+48
    v7 = stack_load.i64 ss1+56
    v8 = iadd v4, v5
    v9 = iadd v6, v7
    v10 = iadd v8, v9
    return v10
}
; run: %avx512_gather_i64x8_i64x8() == 414

function %avx512_gather_i64x8_i32x8() -> i64 {
    ss0 = explicit_slot 64
    ss1 = explicit_slot 64

block0:
    v0 = vconst.i64x8 [100 101 102 103 104 105 106 107]
    stack_store v0, ss0
    v1 = vconst.i32x8 [7 0 6 1 5 2 4 3]
    v2 = stack_addr.i64 ss0
    v3 = x86_simd_gather.i64x8 notrap aligned v2, v1, 8
    stack_store v3, ss1
    v4 = stack_load.i64 ss1
    v5 = stack_load.i64 ss1+8
    v6 = stack_load.i64 ss1+48
    v7 = stack_load.i64 ss1+56
    v8 = iadd v4, v5
    v9 = iadd v6, v7
    v10 = iadd v8, v9
    return v10
}
; run: %avx512_gather_i64x8_i32x8() == 414

function %avx512_scatter_i32x16() -> i32 {
    ss0 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    stack_store v0, ss0
    v1 = vconst.i32x16 [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]
    v2 = vconst.i32x16 [8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7]
    v3 = vconst.i32x16 [0 0 0 0 0 0 0 0 -1 -1 -1 -1 -1 -1 -1 -1]
    v4 = stack_addr.i64 ss0
    x86_simd_scatter.i32x16 notrap aligned v3, v1, v4, v2, 4
    v5 = stack_load.i32 ss0
    v6 = stack_load.i32 ss0+4
    v7 = stack_load.i32 ss0+56
    v8 = stack_load.i32 ss0+60
    v9 = iadd v5, v6
    v10 = iadd v7, v8
    v11 = iadd v9, v10
    return v11
}
; run: %avx512_scatter_i32x16() == 246

function %avx512_scatter_i64x8_i32x8() -> i64 {
    ss0 = explicit_slot 64

block0:
    v0 = vconst.i64x8 [0 1 2 3 4 5 6 7]
    stack_store v0, ss0
    v1 = vconst.i64x8 [100 101 102 103 104 105 106 107]
    v2 = vconst.i32x8 [7 0 6 1 5 2 4 3]
    v3 = vconst.i64x8 [0 0 0 0 -1 -1 -1 -1]
    v4 = stack_addr.i64 ss0
    x86_simd_scatter.i64x8 notrap aligned v3, v1, v4, v2, 8
    v5 = stack_load.i64 ss0+16
    v6 = stack_load.i64 ss0+24
    v7 = stack_load.i64 ss0+32
    v8 = stack_load.i64 ss0+40
    v9 = iadd v5, v6
    v10 = iadd v7, v8
    v11 = iadd v9, v10
    return v11
}
; run: %avx512_scatter_i64x8_i32x8() == 422

function %avx512_compress_i32x16() -> i32 {
    ss0 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16]
    v1 = vconst.i32x16 [-1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0]
    v2 = x86_simd_compress.i32x16 v1, v0
    stack_store v2, ss0
    v3 = stack_load.i32 ss0
    v4 = stack_load.i32 ss0+4
    v5 = stack_load.i32 ss0+24
    v6 = stack_load.i32 ss0+28
    v7 = iadd v3, v4
    v8 = iadd v5, v6
    v9 = iadd v7, v8
    return v9
}
; run: %avx512_compress_i32x16() == 32

function %avx512_expand_i32x16() -> i32 {
    ss0 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [1 3 5 7 9 11 13 15 0 0 0 0 0 0 0 0]
    v1 = vconst.i32x16 [-1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0]
    v2 = x86_simd_expand.i32x16 v1, v0
    stack_store v2, ss0
    v3 = stack_load.i32 ss0
    v4 = stack_load.i32 ss0+4
    v5 = stack_load.i32 ss0+56
    v6 = stack_load.i32 ss0+60
    v7 = iadd v3, v4
    v8 = iadd v5, v6
    v9 = iadd v7, v8
    return v9
}
; run: %avx512_expand_i32x16() == 16

function %avx512_masked_store_i32x16_cmpmask() -> i32 {
    ss0 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    stack_store v0, ss0
    v1 = vconst.i32x16 [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]
    v2 = iconst.i32 8
    v3 = splat.i32x16 v2
    v4 = icmp slt v0, v3
    v5 = stack_addr.i64 ss0
    x86_simd_masked_store.i32x16 notrap aligned v4, v1, v5
    v6 = stack_load.i32 ss0
    v7 = stack_load.i32 ss0+4
    v8 = stack_load.i32 ss0+28
    v9 = stack_load.i32 ss0+32
    v10 = iadd v6, v7
    v11 = iadd v8, v9
    v12 = iadd v10, v11
    return v12
}
; run: %avx512_masked_store_i32x16_cmpmask() == 316

function %avx512_masked_add_i32x16() -> i32 {
    ss0 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16]
    v1 = vconst.i32x16 [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]
    v2 = iconst.i32 9
    v3 = splat.i32x16 v2
    v4 = icmp slt v0, v3
    v5 = iadd v0, v1
    v6 = bitselect v4, v5, v0
    stack_store v6, ss0
    v7 = stack_load.i32 ss0
    v8 = stack_load.i32 ss0+4
    v9 = stack_load.i32 ss0+28
    v10 = stack_load.i32 ss0+32
    v11 = iadd v7, v8
    v12 = iadd v9, v10
    v13 = iadd v11, v12
    return v13
}
; run: %avx512_masked_add_i32x16() == 328

function %avx512_masked_add_i64x8() -> i64 {
    ss0 = explicit_slot 64

block0:
    v0 = vconst.i64x8 [1 2 3 4 5 6 7 8]
    v1 = vconst.i64x8 [100 101 102 103 104 105 106 107]
    v2 = iconst.i64 5
    v3 = splat.i64x8 v2
    v4 = icmp slt v0, v3
    v5 = iadd v0, v1
    v6 = bitselect v4, v5, v0
    stack_store v6, ss0
    v7 = stack_load.i64 ss0
    v8 = stack_load.i64 ss0+8
    v9 = stack_load.i64 ss0+24
    v10 = stack_load.i64 ss0+32
    v11 = iadd v7, v8
    v12 = iadd v9, v10
    v13 = iadd v11, v12
    return v13
}
; run: %avx512_masked_add_i64x8() == 316

function %avx512_mask_chain_i32x16() -> i32 {
    ss0 = explicit_slot 64
    ss1 = explicit_slot 64
    ss2 = explicit_slot 64

block0:
    v0 = vconst.i32x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    v1 = vconst.i32x16 [100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]
    stack_store v1, ss0
    v2 = iconst.i32 8
    v3 = splat.i32x16 v2
    v4 = icmp slt v0, v3
    v5 = bnot v4
    v6 = bxor v4, v5
    v7 = band v4, v5
    v8 = stack_addr.i64 ss0
    v9 = x86_simd_masked_load.i32x16 notrap aligned v4, v0, v8
    v10 = x86_simd_masked_load.i32x16 notrap aligned v5, v1, v8
    v11 = x86_simd_masked_load.i32x16 notrap aligned v7, v0, v8
    v12 = bitselect v4, v9, v10
    v13 = bitselect v6, v12, v11
    v14 = bitselect v7, v12, v11
    stack_store v0, ss1
    v15 = stack_addr.i64 ss1
    x86_simd_masked_store.i32x16 notrap aligned v5, v13, v15
    stack_store v1, ss2
    v16 = stack_addr.i64 ss2
    x86_simd_masked_store.i32x16 notrap aligned v4, v14, v16
    v17 = stack_load.i32 ss1
    v18 = stack_load.i32 ss1+28
    v19 = stack_load.i32 ss1+32
    v20 = stack_load.i32 ss1+60
    v21 = stack_load.i32 ss2
    v22 = stack_load.i32 ss2+28
    v23 = stack_load.i32 ss2+32
    v24 = stack_load.i32 ss2+60
    v25 = iadd v17, v18
    v26 = iadd v19, v20
    v27 = iadd v21, v22
    v28 = iadd v23, v24
    v29 = iadd v25, v26
    v30 = iadd v27, v28
    v31 = iadd v29, v30
    return v31
}
; run: %avx512_mask_chain_i32x16() == 460
